{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f922d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+-------+---------+---------+------------------+----------+--------+-------------------+-------------------+-------------------+----------+--------------+--------------------+--------------------+----------------+----------------+--------------------+\n",
      "|             address|categories|    city|country| latitude|longitude|              name|postalCode|province|       reviews.date|  reviews.dateAdded|reviews.doRecommend|reviews.id|reviews.rating|        reviews.text|       reviews.title|reviews.userCity|reviews.username|reviews.userProvince|\n",
      "+--------------------+----------+--------+-------+---------+---------+------------------+----------+--------+-------------------+-------------------+-------------------+----------+--------------+--------------------+--------------------+----------------+----------------+--------------------+\n",
      "|Riviera San Nicol...|    Hotels|Mableton|     US|45.421611|12.376187|Hotel Russo Palace|     30126|      GA|2013-09-22 05:00:00|2016-10-24 05:00:25|               NULL|      NULL|           4.0|Pleasant 10 min w...|Good location awa...|            NULL|     Russ (kent)|                NULL|\n",
      "|Riviera San Nicol...|    Hotels|Mableton|     US|45.421611|12.376187|Hotel Russo Palace|     30126|      GA|2015-04-03 05:00:00|2016-10-24 05:00:25|               NULL|      NULL|           5.0|Really lovely hot...|Great hotel with ...|            NULL|      A Traveler|                NULL|\n",
      "|Riviera San Nicol...|    Hotels|Mableton|     US|45.421611|12.376187|Hotel Russo Palace|     30126|      GA|2014-05-13 05:00:00|2016-10-24 05:00:25|               NULL|      NULL|           5.0|Ett mycket bra ho...|         Lugnt l��ge|            NULL|            Maud|                NULL|\n",
      "|Riviera San Nicol...|    Hotels|Mableton|     US|45.421611|12.376187|Hotel Russo Palace|     30126|      GA|2013-10-27 05:00:00|2016-10-24 05:00:25|               NULL|      NULL|           5.0|We stayed here fo...|Good location on ...|            NULL|           Julie|                NULL|\n",
      "|Riviera San Nicol...|    Hotels|Mableton|     US|45.421611|12.376187|Hotel Russo Palace|     30126|      GA|2015-03-05 05:00:00|2016-10-24 05:00:25|               NULL|      NULL|           5.0|We stayed here fo...|������ ����������...|            NULL|        sungchul|                NULL|\n",
      "+--------------------+----------+--------+-------+---------+---------+------------------+----------+--------+-------------------+-------------------+-------------------+----------+--------------+--------------------+--------------------+----------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- address: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- postalCode: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- reviews.date: timestamp (nullable = true)\n",
      " |-- reviews.dateAdded: timestamp (nullable = true)\n",
      " |-- reviews.doRecommend: string (nullable = true)\n",
      " |-- reviews.id: string (nullable = true)\n",
      " |-- reviews.rating: double (nullable = true)\n",
      " |-- reviews.text: string (nullable = true)\n",
      " |-- reviews.title: string (nullable = true)\n",
      " |-- reviews.userCity: string (nullable = true)\n",
      " |-- reviews.username: string (nullable = true)\n",
      " |-- reviews.userProvince: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hotel Recommendation System\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv('hotel_reviews.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the dataset to understand what data looks like\n",
    "df.show(5)\n",
    "\n",
    "# Print the schema to understand the data types and column names\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92844ee2",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c79577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- postalCode: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- reviews_date: timestamp (nullable = true)\n",
      " |-- reviews_dateAdded: timestamp (nullable = true)\n",
      " |-- reviews_doRecommend: string (nullable = true)\n",
      " |-- reviews_id: string (nullable = true)\n",
      " |-- reviews_rating: double (nullable = true)\n",
      " |-- reviews_text: string (nullable = true)\n",
      " |-- reviews_title: string (nullable = true)\n",
      " |-- reviews_userCity: string (nullable = true)\n",
      " |-- reviews_username: string (nullable = true)\n",
      " |-- reviews_userProvince: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming you've already loaded the dataset into DataFrame df\n",
    "# Rename columns to replace '.' with '_'\n",
    "for column in df.columns:\n",
    "    df = df.withColumnRenamed(column, column.replace('.', '_'))\n",
    "\n",
    "# Show the updated schema to confirm changes\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5e10611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+-------+--------+---------+----+----------+--------+------------+-----------------+-------------------+----------+--------------+------------+-------------+----------------+----------------+--------------------+\n",
      "|address|categories|city|country|latitude|longitude|name|postalCode|province|reviews_date|reviews_dateAdded|reviews_doRecommend|reviews_id|reviews_rating|reviews_text|reviews_title|reviews_userCity|reviews_username|reviews_userProvince|\n",
      "+-------+----------+----+-------+--------+---------+----+----------+--------+------------+-----------------+-------------------+----------+--------------+------------+-------------+----------------+----------------+--------------------+\n",
      "|      0|         0|   0|      0|      86|       86|   0|        55|       0|         259|                0|              35912|     35912|           862|          20|         1612|           19581|              75|               18392|\n",
      "+-------+----------+----+-------+--------+---------+----+----------+--------+------------+-----------------+-------------------+----------+--------------+------------+-------------+----------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, isnull, count\n",
    "\n",
    "# Check for missing values in each column\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c07ec3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+-------+--------+---------+----+----------+--------+------------+-----------------+--------------+------------+-------------+----------------+\n",
      "|address|categories|city|country|latitude|longitude|name|postalCode|province|reviews_date|reviews_dateAdded|reviews_rating|reviews_text|reviews_title|reviews_username|\n",
      "+-------+----------+----+-------+--------+---------+----+----------+--------+------------+-----------------+--------------+------------+-------------+----------------+\n",
      "|      0|         0|   0|      0|      76|       76|   0|        55|       0|         257|                0|             0|           0|            0|              75|\n",
      "+-------+----------+----+-------+--------+---------+----+----------+--------+------------+-----------------+--------------+------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with a large number of missing values\n",
    "df = df.drop(\"reviews_doRecommend\", \"reviews_id\", \"reviews_userCity\", \"reviews_userProvince\")\n",
    "\n",
    "# Drop rows where 'reviews_text' or 'reviews_rating' is null as these are crucial for analysis\n",
    "df = df.na.drop(subset=[\"reviews_text\", \"reviews_rating\"])\n",
    "\n",
    "# You can opt to fill missing 'reviews_title' with 'No Title' if not crucial\n",
    "df = df.na.fill({\"reviews_title\": \"No Title\"})\n",
    "\n",
    "# Show updated counts of missing values for verification\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a7efb",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6457791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a UDF to compute sentiment category\n",
    "def sentiment_analysis(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "sentiment_udf = udf(sentiment_analysis, StringType())\n",
    "\n",
    "# Apply the sentiment analysis UDF to the review texts\n",
    "df = df.withColumn(\"sentiment_score\", sentiment_udf(df.reviews_text))\n",
    "df = df.withColumn(\"sentiment_category\", when(col(\"sentiment_score\") > 0, \"Positive\")\n",
    "                                         .when(col(\"sentiment_score\") == 0, \"Neutral\")\n",
    "                                         .otherwise(\"Negative\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5000c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "# Add a new column that represents the length of each review\n",
    "df = df.withColumn(\"review_length\", length(df.reviews_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edf07b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+-------+---------+---------+------------------+----------+--------+-------------------+-------------------+--------------+--------------------+--------------------+----------------+------------------+------------------+-------------+\n",
      "|             address|categories|    city|country| latitude|longitude|              name|postalCode|province|       reviews_date|  reviews_dateAdded|reviews_rating|        reviews_text|       reviews_title|reviews_username|   sentiment_score|sentiment_category|review_length|\n",
      "+--------------------+----------+--------+-------+---------+---------+------------------+----------+--------+-------------------+-------------------+--------------+--------------------+--------------------+----------------+------------------+------------------+-------------+\n",
      "|Riviera San Nicol...|    Hotels|Mableton|     US|45.421611|12.376187|Hotel Russo Palace|     30126|      GA|2013-09-22 05:00:00|2016-10-24 05:00:25|           4.0|Pleasant 10 min w...|Good location awa...|     Russ (kent)|0.5208333333333333|           Neutral|          194|\n",
      "|Riviera San Nicol...|    Hotels|Mableton|     US|45.421611|12.376187|Hotel Russo Palace|     30126|      GA|2015-04-03 05:00:00|2016-10-24 05:00:25|           5.0|Really lovely hot...|Great hotel with ...|      A Traveler|0.6357142857142858|           Neutral|          252|\n",
      "|Riviera San Nicol...|    Hotels|Mableton|     US|45.421611|12.376187|Hotel Russo Palace|     30126|      GA|2014-05-13 05:00:00|2016-10-24 05:00:25|           5.0|Ett mycket bra ho...|         Lugnt l��ge|            Maud|              0.35|           Neutral|          136|\n",
      "+--------------------+----------+--------+-------+---------+---------+------------------+----------+--------+-------------------+-------------------+--------------+--------------------+--------------------+----------------+------------------+------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba4a51",
   "metadata": {},
   "source": [
    "## Feature Standardization for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e3fbb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[\"reviews_rating\", \"review_length\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(df)\n",
    "df = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123dc8ff",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fc6cd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         3|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         2|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(featuresCol='scaledFeatures', k=5)\n",
    "model = kmeans.fit(df)\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# Show the results\n",
    "predictions.select(\"prediction\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f4b85",
   "metadata": {},
   "source": [
    "# Developing a Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c9240",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ab040b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 01:22:00 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error before tuning = 2.0850366333726944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4850:>                                                       (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model Root-mean-square error = 1.4421299497904674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hash\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Initialize Spark session (assuming it's already done in your environment)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Hotel Recommendation System\").getOrCreate()\n",
    "\n",
    "# Load the data (assuming it's already loaded into DataFrame df)\n",
    "\n",
    "# Convert usernames and hotel names to numeric IDs using Spark's hash function\n",
    "df = df.withColumn(\"userId\", hash(\"reviews_username\"))\n",
    "df = df.withColumn(\"hotelId\", hash(\"name\"))\n",
    "\n",
    "# Split data into training and test sets\n",
    "(training, test) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"hotelId\", ratingCol=\"reviews_rating\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"reviews_rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error before tuning = \" + str(rmse))\n",
    "\n",
    "# Parameter tuning using cross-validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20, 50]) \\\n",
    "    .addGrid(als.maxIter, [5, 10]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.05, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=als,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"reviews_rating\", predictionCol=\"prediction\"),\n",
    "                          numFolds=3)  # Use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters\n",
    "cvModel = crossval.fit(training)\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "bestPredictions = bestModel.transform(test)\n",
    "bestRmse = evaluator.evaluate(bestPredictions)\n",
    "print(\"Best model Root-mean-square error = \" + str(bestRmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e7670",
   "metadata": {},
   "source": [
    "## Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abbb54a",
   "metadata": {},
   "source": [
    "### Step 1: Feature Extraction Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6915147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       tfidfFeatures|\n",
      "+--------------------+\n",
      "|(1000,[17,49,56,1...|\n",
      "|(1000,[16,17,25,1...|\n",
      "|(1000,[14,77,99,1...|\n",
      "|(1000,[16,17,50,5...|\n",
      "|(1000,[16,17,50,5...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Text processing to convert reviews into features\n",
    "tokenizer = Tokenizer(inputCol=\"reviews_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidfFeatures\")\n",
    "\n",
    "# Pipeline to process text\n",
    "text_pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "model = text_pipeline.fit(df)\n",
    "featured_data = model.transform(df)\n",
    "\n",
    "# Displaying the transformed DataFrame to check new columns\n",
    "featured_data.select(\"tfidfFeatures\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d7947",
   "metadata": {},
   "source": [
    "### Step 2: Calculating Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55bdac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Normalize the feature vectors\n",
    "normalizer = Normalizer(inputCol=\"tfidfFeatures\", outputCol=\"normFeatures\")\n",
    "normalized_data = normalizer.transform(featured_data)\n",
    "\n",
    "# Self-join to compute pairwise similarities (conceptual, adjust based on resources)\n",
    "# This might be very large; consider using approximations or limiting the number of comparisons\n",
    "pairwise_similarity = normalized_data.alias(\"hotels1\").join(normalized_data.alias(\"hotels2\"), \"hotelId\") \\\n",
    "    .select(\n",
    "        col(\"hotels1.name\").alias(\"hotel1\"),\n",
    "        col(\"hotels2.name\").alias(\"hotel2\"),\n",
    "        col(\"hotels1.normFeatures\").alias(\"features1\"),\n",
    "        col(\"hotels2.normFeatures\").alias(\"features2\")\n",
    "    )\n",
    "\n",
    "# Define a UDF to calculate cosine similarity between feature vectors\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(features1, features2):\n",
    "    return float(np.dot(features1, features2) / (np.linalg.norm(features1) * np.linalg.norm(features2)))\n",
    "\n",
    "similarity_udf = udf(cosine_similarity, FloatType())\n",
    "\n",
    "pairwise_similarity = pairwise_similarity.withColumn(\"similarity\", similarity_udf(col(\"features1\"), col(\"features2\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f95c3",
   "metadata": {},
   "source": [
    "### Step 3: Recommend Based on Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b60c3717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5016:==========================================>             (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+--------------------+----------+\n",
      "|            hotel1|            hotel2|           features1|           features2|similarity|\n",
      "+------------------+------------------+--------------------+--------------------+----------+\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[16,17,21,7...|(1000,[16,17,21,7...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[48,76,139,...|(1000,[48,76,139,...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[18,28,30,3...|(1000,[18,28,30,3...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[18,28,83,9...|(1000,[18,28,83,9...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[16,17,50,5...|(1000,[16,17,50,5...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[0,1,40,55,...|(1000,[0,1,40,55,...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[14,77,99,1...|(1000,[14,77,99,1...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[16,17,50,5...|(1000,[16,17,50,5...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[17,49,56,1...|(1000,[17,49,56,1...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[16,17,50,5...|(1000,[16,17,50,5...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[53,100,152...|(1000,[53,100,152...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[17,154,185...|(1000,[17,154,185...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[95,126,257...|(1000,[95,126,257...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[1,24,30,49...|(1000,[1,24,30,49...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|  (1000,[828],[1.0])|  (1000,[828],[1.0])|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[52,70,73,1...|(1000,[52,70,73,1...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|  (1000,[828],[1.0])|  (1000,[828],[1.0])|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[16,17,50,5...|(1000,[16,17,50,5...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[2,49,101,1...|(1000,[2,49,101,1...|       1.0|\n",
      "|Hotel Russo Palace|Hotel Russo Palace|(1000,[12,17,126,...|(1000,[12,17,126,...|       1.0|\n",
      "+------------------+------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter to show recommendations for a specific hotel\n",
    "# For instance, recommendations for the first hotel in the DataFrame\n",
    "target_hotel = pairwise_similarity.filter(col(\"hotel1\") == \"Hotel Russo Palace\")\n",
    "top_recommendations = target_hotel.sort(col(\"similarity\").desc()).limit(50)\n",
    "\n",
    "top_recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66535f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
